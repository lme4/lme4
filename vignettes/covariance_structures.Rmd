---
title: "Covariance Structures"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Covariance Structures}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{glmmTMB}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Introduction

The current version of `lme4` offers four covariance classes/structures: `Covariance.us` (unstructured), `Covariance.diag` (diagonal), `Covariance.cs` (compound symmetry), and `Covariance.ar1` (autoregressive order 1). The syntax for use is somewhat similar to the `glmmTMB` package (see [covariance structures in glmmTMB](https://glmmtmb.github.io/glmmTMB/articles/covstruct.html)), although the results are slightly different.

The first half of this vignette provide a more detailed explanation of how the 
new machinery works, and the second half of the vignette will compare `lme4` 
and `glmmTMB` implementations and results.

### Background

For exact details of this structure, refer to the `lmer` [vignette](https://cran.r-project.org/package=lme4/vignettes/lmer.pdf). We provide
a quick summary in this vignette.

In matrix notation, a linear mixed model can be represented as:
\[
\mathbf{y} = \mathbf{X} \beta + Z \mathbf{b} + \boldsymbol{\epsilon}
\]
Where $\mathbf{b}$ represents an unknown vector of random effects. The 
\code{Covariance} class helps define the structure of the variance-covariance 
matrix as specified as $\textrm{Var}(\mathbf{b}$. Typically, we denote the 
variance-covariance matrix as $\Sigma$.

First, we create the relative co-factor $\Lambda_{\theta}$ which is a 
$q \times q$ block diagonal  matrix that depends on the variance-component 
parameter $\theta$. Let $\sigma$ be the scale parameter of the variance of a 
linear mixed model. In `lme4`, the variance-covariance matrix is constructed by
the following equation:
\[
\mathbf{\Sigma}_{\mathbf{\theta}} = \sigma^{2} \Lambda_{\theta} 
  \Lambda_{\theta}^{T},
\]

The major difference between the four covariance classes (`Covariance.us` (unstructured), `Covariance.diag` (diagonal), `Covariance.cs` (compound symmetry), and `Covariance.ar1` (autoregressive order 1)) is the construction of the the relative Cholesky factor $\Lambda_{\theta}$.

### Covariance Structures

Suppose there are $p$ number of random effect terms for a particular
grouping variable. The **unstructured covariance structure**, which is the default 
in `lme4`, of size $p \times p$ has the following form:
\[
\mathbf{\Sigma}
= \begin{bmatrix}
\sigma^{2}_{1} & \sigma_{12} & \dots & \sigma_{1p} \\
\sigma_{21} & \sigma^{2}_{2} & \dots & \sigma_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{p1} & \sigma_{p2} & \dots & \sigma^{2}_{p} \\
\end{bmatrix}	
\]

The next three covariance structures can either be heterogeneous or homogeneous.
If we have a homogeneous covariance structure (`hom = TRUE`), then we assume 
$\sigma_{1} = \sigma_{2} = \dots = \sigma_{p}$.

The **diagonal covariance structure** has the following form:
\[
\mathbf{\Sigma}
= \begin{bmatrix}
\sigma^{2}_{1} & 0 & \dots & 0 \\
0 & \sigma^{2}_{2} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma^{2}_{p} \\
\end{bmatrix}	
\]
By default, we assume a heterogeneous diagonal covariance structure.

The **compound symmetric covariance structure** has the following form:
\[
\mathbf{\Sigma}
= \begin{bmatrix}
\sigma^{2}_{1}              & \sigma_{1}\sigma_{2}\rho  & \sigma_{1}\sigma_{3}\rho  & \dots  & \sigma_{1}\sigma_{p}\rho \\
\sigma_{2}\sigma_{1} \rho   & \sigma^{2}_{2}             & \sigma_{2}\sigma_{3}\rho  & \dots  & \sigma_{2}\sigma_{p}\rho \\
\sigma_{3}\sigma_{1} \rho   & \sigma_{3}\sigma_{2}\rho   & \sigma^{2}_{3}            & \dots  & \sigma_{3}\sigma_{p}\rho \\
\vdots                      & \vdots                     & \vdots                    & \ddots & \vdots \\
\sigma_{p}\sigma_{1} \rho   & \sigma_{p}\sigma_{2}\rho   & \sigma_{p}\sigma_{3}\rho  & \dots  & \sigma^{2}_{p} 
\end{bmatrix}	
\]
By default, we assume a heterogeneous compound symmetric covariance structure.

The **AR1 (auto-regressive order 1) covariance structure** has the following form:
\[
\mathbf{\Sigma}
= \begin{bmatrix}
\sigma^{2}_{1}                  & \sigma_{1}\sigma_{2} \rho      & \sigma_{1}\sigma_{3}\rho^{2}   & \dots  & \sigma_{1}\sigma_{p}\rho^{p-1} \\
\sigma_{2}\sigma_{1} \rho       & \sigma^{2}_{2}                 & \sigma_{2}\sigma_{3}\rho       & \dots  & \sigma_{2}\sigma_{p}\rho^{p-2} \\
\sigma_{3}\sigma_{1} \rho^{2}   & \sigma_{3}\sigma_{2}\rho       & \sigma^{2}_{3}                 & \dots  & \sigma_{3}\sigma_{p}\rho^{p-3} \\
\vdots                          & \vdots                         & \vdots                         & \ddots & \vdots \\
\sigma_{p}\sigma_{1} \rho^{p-1} & \sigma_{p}\sigma_{2}\rho^{p-2} & \sigma_{p}\sigma_{3}\rho^{p-3} & \dots  & \sigma^{2}_{p} 
\end{bmatrix}	
\]
Unlike the diagonal and compound symmetric structures, by default we assume a
homogeneous ar1 covariance structure.

### Construction of the Relative Co-factor

For the **unstructured covariance matrix**, the following parameters `par`:
$\boldsymbol{\theta} = (\theta_{1}, \theta_{2}, \dots, \theta_{p(p+1)/2})$ are 
estimated by `lme4`'s compiled C++ code (using Eigen) to construct the relative
co-factor $\Lambda_{\boldsymbol{\theta}}$:
\[
\Lambda_{\boldsymbol{\theta}} = \begin{bmatrix}
\theta_1 & 0 & 0 & \dots & 0 \\
\theta_2 & \theta_3 & 0 & \dots & 0 \\
\theta_4 & \theta_5 & \theta_6 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\theta_{...} & \theta_{...} & \theta_{...} & \dots & \theta_{p(p+1)/2}
\end{bmatrix}
\]

It differs for the other covariance structures. In the **diagonal covariance matrix** 
case, $\boldsymbol{\theta}$ (or `par`) only contains the standard deviations. 
The relative co-factor $\Lambda_{\boldsymbol{\theta}}$ is:
\[
\Lambda_{\boldsymbol{\theta}} = \begin{bmatrix}
\theta_1 & 0 & 0 & \dots & 0 \\
0 & \theta_2 & 0 & \dots & 0 \\
0 & 0 & \theta_3 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \theta_{p}
\end{bmatrix}
\]

For the **compound symmetry covariance structure**, the parameter 
vector `par` contains the $p$ standard deviations $(\sigma_1, \sigma_2, \ldots, \sigma_p)$ 
and the common correlation $\rho$.

The relative co-factor $\Lambda_{\boldsymbol{\theta}}$ is a lower triangular $p \times p$ matrix. Consider the form:
\[
\Lambda_{\boldsymbol{\theta}} = \begin{bmatrix}
\theta_{11} & 0 & 0 & \cdots & 0 \\
\theta_{21} & \theta_{22} & 0 & \cdots & 0 \\
\theta_{31} & \theta_{32} & \theta_{33} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\theta_{p1} & \theta_{p2} & \theta_{p3} & \cdots & \theta_{pp}
\end{bmatrix}
\]

Its elements $\theta_{ij}$ are constructed as follows. First, define the 
sequence $\{a_j\}$ recursively:
\[
a_1 = 0, \quad a_{j+1} = a_j + \frac{(1 - \rho \cdot a_j)^2}{1 - \rho^2 \cdot a_j} \quad \text{for } j = 1, \ldots, p-1
\]

Then the elements of $\Lambda$ are given by:
\[
\theta_{ij} = \begin{cases}
 \sqrt{1 - \rho^2 a_j} & \text{if } i = j \text{ (diagonal)} \\[0.5em]
 \dfrac{\rho - \rho^2 a_j}{\sqrt{1 - \rho^2 a_j}} & \text{if } i > j \text{ (below diagonal)} \\[0.5em]
0 & \text{if } i < j \text{ (above diagonal)}
\end{cases}
\]
In the code, one can extract the values $\theta_{ij}$ via the `getTheta()` function 
of the `Covariance.cs` object, in which `theta` will be a vector in the column-wise 
elements of $\Lambda$.
 
It is similar for the **autoregressive order 1 (AR1) covariance structure**. 
Again, the parameter vector contains the $p$ standard deviations 
$(\sigma_1, \sigma_2, \ldots, \sigma_p)$ and the autocorrelation parameter 
$\rho$. The relative co-factor $\Lambda_{\boldsymbol{\theta}}$ is a lower 
triangular $p \times p$ matrix whose form is similar to the compound symmetric case.

The elements $\theta_{ij}$ are given by:
\[
\theta_{ij} = \begin{cases}
 \rho^{i-1} & \text{if } j = 1 \text{ (first column)} \\[0.5em]
 \rho^{i-j} \sqrt{1 - \rho^2} & \text{if } 1 < j \leq i \text{ (below diagonal)} \\[0.5em]
0 & \text{if } i < j \text{ (above diagonal)}
\end{cases}
\]
Again, these values can easily be extracted using `getTheta()` function 
of the `Covariance.ar1` object, in which `theta` will be still be a vector in 
the column-wise elements of $\Lambda$.

### Example of Use

In this section we will mostly discuss functions relating to extracting 
`par`, `theta`, `Lambda`, as described in the previous section, as well as
extracting the variance covariance matrix.

The next section, where we compare the results from `lme4` against another popular `R`
mixed modelling package, `glmmTMB`, is where there will be plenty of examples of use of
how to use each of the different covariance structures, as well as their `glmmTMB` equivalents 
(as different covariance structures have been implemented in `glmmTMB` earlier.)

Consider the classical `sleepstudy` example, except we are going to impose a
compound symmetric covariance structure to highlight the differences between
`par` and `theta`:

```{r setup}
library(lme4)
fm1.cs <- lmer(Reaction ~ Days + cs(Days | Subject), sleepstudy)
```

The information regarding the covariance structure is included within the `reCovs` slot:

```{r}
print(fm1.cs_cov <- lme4:::getReCovs(fm1.cs))
```

It only contains one item as we only have one grouping variable (`Subject`). To 
see what `par`, `theta`, and `Lambda` of this object, we can call:

```{r}
getME(fm1.cs, "par")
getME(fm1.cs, "theta")
getME(fm1.cs, "Lambda")
```

To most users, the most crucial information is simply extracting the variance 
covariance matrix. This can easily be extracted via `VarCorr.merMod()`:

```{r}
vc_mat <- VarCorr(fm1.cs)
vc_mat$Subject
```

## Comparisons with glmmTMB

### Reason for Computational Differences

When constructing generalized linear mixed models, `lme4` uses the Cholesky decomposition to reduce the problem to a function of the covariance parameters. In particular, `lme4` parameterizes the random-effects varianceâ€“covariance matrix $$
\Sigma_{\theta} = \sigma^{2} \Lambda_{\theta} \Lambda_{\theta}^{T},
$$ where the relative covariance factor (the Cholesky factor) $\Sigma_{\theta}$ is lower-triangular with positive diagonal entries, and is dependent on some variance-component parameter $\theta$. (For exact details of this structure, refer to the `lmer` [vignette](https://cran.r-project.org/package=lme4/vignettes/lmer.pdf).)

This allows optimization to proceed over the unconstrained elements of $\Sigma_{\theta}$, ensuring a gradient-free nonlinear optimization of the covariance parameter $\theta$'s.

Meanwhile, `glmmTMB` uses direct maximum likelihood estimation via Template Model Builder (TMB). Fixed effects ($\beta$'s), covariance parameters ($\theta$'s) and dispersion parameters are profiled out, and optimization proceeds on a likelihood parameterized by said fixed effects, covariance parameters, dispersion parameters, etc.

Despite these differences, we will show examples where `lme4` and `glmmTMB` provide similar estimates when they both use maximum likelihood estimation. By default, `lme4` uses the restricted maximum likelihood; hence in the following examples, we use `lmer(..., REML = FALSE)` to compare against `glmmTMB`.

### Comparison Setup

```{r}
if (!requireNamespace("glmmTMB", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(glmmTMB)
}

## Often want to ignore attributes and class.
all.equal.nocheck <- function(x, y, ..., check.attributes = FALSE, check.class = FALSE) {
  require("Matrix", quietly = TRUE)
  ## working around mode-matching headaches
  if (is(x, "Matrix")) x <- matrix(x)
  if (is(y, "Matrix")) y <- matrix(y)
  all.equal(x, y, ..., check.attributes = check.attributes, check.class = check.class)
}

get.cor1 <- function(x) {
  v <- VarCorr(x)
  vv <- if (inherits(x, "merMod")) v$group else v$cond$group
  attr(vv, "correlation")[1,2]
}
```

### Unstructured (General Positive Definite)

This is the default setting for both `lme4` and `glmmTMB`. Below we simulate a dataset with known `beta`, `theta` and `sigma` values.

```{r}
n_groups <- 20
n_per_group <- 20
n <- n_groups * n_per_group

set.seed(1)
dat.us <- data.frame(
  group = rep(1:n_groups, each = n_per_group),
  x1 = rnorm(n),
  x2 = rnorm(n)
)
## Constructing a similar dataset for the other examples
gdat.us <- dat.diag <- gdat.diag <- dat.us

form <- y ~ 1 + x1 * x2 + us(1 + x1|group)
dat.us$y <- simulate(form[-2], 
                    newdata = dat.us,
                    family = gaussian,
                    newparams = list(beta = c(-7, 5, -100, 20),
                                     theta = c(2.5, 1.4, 6.3),
                                     sigma = 2))[[1]]

form2 <- y ~ 1 + x1 + us(1 + x1|group)
gdat.us$y <- simulate(
  form2[-2],
  newdata = gdat.us,
  family = binomial,
  newparams = list(
    beta  = c(-1.5, 0.5),     
    theta = c(0.3, 0.1, 0.3)
  ))[[1]]
```


### Linear Mixed Model

```{r}
lme4.us <- lmer(form, data = dat.us, REML = "FALSE")
glmmTMB.us <- glmmTMB(form, dat = dat.us)

## Fixed effects
fixef(lme4.us); fixef(glmmTMB.us)$cond
all.equal.nocheck(fixef(lme4.us), fixef(glmmTMB.us)$cond)

## Sigma
sigma(lme4.us); sigma(glmmTMB.us)
all.equal.nocheck(sigma(lme4.us), sigma(glmmTMB.us))

## Log likelihoods
logLik(lme4.us); logLik(glmmTMB.us)
all.equal.nocheck(logLik(lme4.us), logLik(glmmTMB.us))
```

As expected, from here-on out, calculations related to the random-effects term differ.

```{r}
## Variance-Covariance Matrix
vcov(lme4.us); vcov(glmmTMB.us)$cond
all.equal.nocheck(vcov(lme4.us), vcov(glmmTMB.us)$cond)

## Variance and Covariance Components
all.equal.nocheck(VarCorr(lme4.us)$group,
          VarCorr(glmmTMB.us)$cond$group)

## Conditional Modes of the Random Effects
all.equal.nocheck(ranef(lme4.us)$group, ranef(glmmTMB.us)$cond$group)
```

### Generalized Linear Mixed Model

```{r}
glme4.us <- glmer(form2, data = gdat.us, family = binomial)
gglmmTMB.us <- glmmTMB(form2, dat = gdat.us, family = binomial)

## Fixed effects
fixef(glme4.us); fixef(gglmmTMB.us)$cond
all.equal.nocheck(fixef(glme4.us), fixef(gglmmTMB.us)$cond)

## Sigma
all.equal.nocheck(sigma(glme4.us), sigma(gglmmTMB.us))

## Log likelihoods
logLik(glme4.us); logLik(gglmmTMB.us)
all.equal.nocheck(logLik(glme4.us), logLik(gglmmTMB.us))
```

As expected, from here-on out, calculations related to the random-effects term differ.

```{r}
## Variance-Covariance Matrix
vcov(glme4.us); vcov(gglmmTMB.us)$cond
all.equal.nocheck(vcov(glme4.us), vcov(gglmmTMB.us)$cond)

## Variance and Covariance Components
all.equal.nocheck(VarCorr(glme4.us)$group,
          VarCorr(gglmmTMB.us)$cond$group)

## Conditional Modes of the Random Effects
all.equal.nocheck(ranef(glme4.us)$group, ranef(gglmmTMB.us)$cond$group)
```

### Diagonal

The syntax is the same for fitting a heterogeneous diagonal covariance structure for `lme4` and `glmmTMB`. It differs when we want to fit a *homogeneous* diagonal covariance structure.

To demonstrate, if we wanted to fit a homogeneous diagonal covariance structure then we would write the following:

```{r, eval = FALSE}
lme4.us <- lmer(Reaction ~ Days + diag(Days | Subject, hom = TRUE), sleepstudy)
glmmTMB.us <- glmmTMB(Reaction ~ Days + homdiag(Days | Subject), sleepstudy)
```

Our focus will be the comparisons when we have a *heterogeneous* diagonal covariance structure as those cases are more interesting.

```{r}
form <- y ~ 1 + x1 * x2 + diag(1|group)
dat.diag$y <- simulate(form[-2], 
                       newdata = dat.diag,
                       family = gaussian,
                       newparams = list(beta = c(-7, 5, -100, 20),
                                        theta = c(2.5),
                                        sigma = 2))[[1]]
```


```{r}
lme4.diag <- lmer(form, data = dat.diag, REML = "FALSE")
glmmTMB.diag <- glmmTMB(form, dat = dat.diag)

## Fixed effects
fixef(lme4.diag); fixef(glmmTMB.diag)$cond
all.equal.nocheck(fixef(lme4.diag), fixef(glmmTMB.diag)$cond)

## Sigma
sigma(lme4.diag); sigma(glmmTMB.diag)
all.equal.nocheck(sigma(lme4.diag), sigma(glmmTMB.diag))

## Log likelihoods
logLik(lme4.diag); logLik(glmmTMB.diag)
all.equal.nocheck(logLik(lme4.diag), logLik(glmmTMB.diag))

## Variance-Covariance Matrix
vcov(lme4.diag); vcov(glmmTMB.diag)$cond
all.equal.nocheck(vcov(lme4.diag), vcov(glmmTMB.diag)$cond)

## Variance and Covariance Components
all.equal.nocheck(VarCorr(lme4.diag)[[1]], 
          VarCorr(glmmTMB.diag)$cond$group)

## Conditional Modes of the Random Effects
all.equal.nocheck(ranef(lme4.diag)$group, ranef(glmmTMB.diag)$cond$group)
```

### Compound Symmetry

Similar to the diagonal case, the syntax is the same for fitting a heterogeneous compound symmetry covariance structure for `lme4` and `glmmTMB`: 

```{r, eval = FALSE}
lme4.us <- lmer(Reaction ~ Days + cs(Days | Subject, hom = TRUE), sleepstudy)
glmmTMB.us <- glmmTMB(Reaction ~ Days + cs(Days | Subject), sleepstudy)
```

Again, it differs when we want to fit a *homogeneous* compound symmetry covariance structure, which we will use for our comparisons.

```{r}
simGroup <- function(g, n=6, phi=0.6) {
  x <- MASS::mvrnorm(mu = rep(0,n),
                     Sigma = phi^as.matrix(dist(1:n)) )  
  y <- x + rnorm(n)                              
  times <- factor(1:n)
  group <- factor(rep(g,n))
  data.frame(y, times, group)
}

set.seed(1)
dat.cs <- do.call("rbind", lapply(1:2000, simGroup))
```

```{r}
lme4.cs <- lmer(y ~ times + cs(0 + times | group, hom = TRUE), data = dat.cs, REML = FALSE)
glmmTMB.cs <- glmmTMB(y ~ times + homcs(0 + times | group), data = dat.cs)

## Fixed effects
fixef(lme4.cs); fixef(glmmTMB.cs)$cond
all.equal.nocheck(fixef(lme4.cs), fixef(glmmTMB.cs)$cond)

## Sigma
sigma(lme4.cs); sigma(glmmTMB.cs)
all.equal.nocheck(sigma(lme4.cs), sigma(glmmTMB.cs))

## Log likelihoods
logLik(lme4.cs); logLik(glmmTMB.cs)
all.equal.nocheck(logLik(lme4.cs), logLik(glmmTMB.cs))

## Variance-Covariance Matrix
all.equal.nocheck(vcov(lme4.cs), vcov(glmmTMB.cs)$cond)

## Variance and Covariance Components
all.equal.nocheck(VarCorr(lme4.cs)[[1]], 
          VarCorr(glmmTMB.cs)$cond$group)

## Conditional Modes of the Random Effects
all.equal.nocheck(ranef(lme4.cs)$group, ranef(glmmTMB.cs)$cond$group)

## Comparing against the predicted rho value
lme4.rho <- get.cor1(lme4.cs)
glmmTMB.rho <- get.cor1(glmmTMB.cs)
lme4.rho; glmmTMB.rho
all.equal.nocheck(lme4.rho, glmmTMB.rho)
```

### Autoregressive Order 1

For this comparison, we will focus on a simulated data set where we explicitly denote $\rho = 0.7$ to better compare the results between the two models. 

```{r}
set.seed(1)
dat.ar1 <- do.call("rbind", lapply(1:2000, function(g) simGroup(g, phi = 0.7)))
```

Unlike the diagonal and compound symmetry case, the syntax is different for fitting both a heterogeneous and homogeneous ar1 model for `lme4` and `glmmTMB`.

For a *heterogeneous* ar1 covariance structure we would write the following:

```{r, eval = FALSE}
lme4.ar1 <- lmer(y ~ times + ar1(0 + times | group), data = dat.ar1, REML = FALSE)
glmmTMB.ar1 <- glmmTMB(y ~ times + hetar1(0 + times | group), data = dat.ar1)
```

We will instead focus on comparisons for a *homogeneous* ar1 covariance structure.

```{r}
lme4.ar1 <- lmer(y ~ times + ar1(0 + times | group, hom = TRUE), data = dat.ar1, REML = FALSE)
glmmTMB.ar1 <- glmmTMB(y ~ times + ar1(0 + times | group), data = dat.ar1)

## Fixed effects
fixef(lme4.ar1); fixef(glmmTMB.ar1)$cond
all.equal.nocheck(fixef(lme4.ar1), fixef(glmmTMB.ar1)$cond)

## Sigma
sigma(lme4.ar1); sigma(glmmTMB.ar1)
all.equal.nocheck(sigma(lme4.ar1), sigma(glmmTMB.ar1))

## Log likelihoods
logLik(lme4.ar1); logLik(glmmTMB.ar1)
all.equal.nocheck(logLik(lme4.ar1), logLik(glmmTMB.ar1))

## Variance-Covariance Matrix
all.equal.nocheck(vcov(lme4.ar1), vcov(glmmTMB.ar1)$cond)

## Variance and Covariance Components
all.equal.nocheck(VarCorr(lme4.ar1)$group, 
                  VarCorr(glmmTMB.ar1)$cond$group)

## Conditional Modes of the Random Effects
all.equal.nocheck(ranef(lme4.ar1)$group, ranef(glmmTMB.ar1)$cond$group)

## Comparing against the predicted rho value
lme4.ar1.rho <- get.cor1(lme4.ar1)
glmmTMB.ar1.rho <- get.cor1(glmmTMB.ar1)
lme4.ar1.rho; glmmTMB.ar1.rho
all.equal.nocheck(lme4.ar1.rho, glmmTMB.ar1.rho)
```
